---
layout: post
title: "神经元网络结构"
tag: ai
category: note
---

## 生物神经元结构

神经元包括：细胞体、树突、轴突等

- 细胞体：有细胞核、细胞质、细胞膜等。每个细胞体有一个细胞核，进行呼吸和新陈代谢等生化过程。
- 树突：细胞的输入端
- 轴突：细胞的输出端，通过轴突末梢连接其他神经元

神经冲动只能由前一级神经元轴突末梢传向下一级神经元树突/细胞体，不能反向传递。

![神经元](\assets\ai_0.png)

神经元具有两种工作状态：

- 传入神经冲动使细胞膜电位高于阈值：细胞进入兴奋状态，产生神经冲动
- 传入神经冲动使细胞膜电位低于阈值：细胞进入抑制状态，没有神经冲动

## 神经元数学模型

神经元的数学模型包含三个部分：加权求和、线性动态系统、非线性函数映射

### 加权求和

记$y_i (t)$为第$i$个神经元输出，$\theta_i$为第$i$个神经元阈值，$u_k(t)$为外部输入，$a_{ij}$和$b_{ij}$为权值，则

$$
v_i(t)=\sum_{j=1}^Na_{ij}y_j{t}+\sum_{k=1}^Mb_{ik}u_{k}-\theta_i
$$

矩阵形式：

$$
\boldsymbol{V}(t)=\boldsymbol{A}\boldsymbol{Y}(t)+\boldsymbol{B}\boldsymbol{U}(t)-\boldsymbol{\theta}
$$

### 线性动态系统

传递函数为：

$$
X_i(s)=H(s)V_i(s)
$$

其中$H(s)$常取$1$, $\frac{1}{s}$, $\mathrm{e}^{-Ts}$等

### 非线性函数映射

最常用的函数包括：

- 阶跃函数

$$
f(x_i)=\begin{cases}
1 & x_i\ge0\\
0 & x_i < 0
\end{cases}
$$

或

$$
f(x_i)=\begin{cases}
1 & x_i \ge 0\\
-1 & x_i < 0
\end{cases}
$$

## 神经网络结构和工作方式

### 人工神经网络类型

- 前馈型：神经元接受前一层输入，输出给下一层，没有反馈。
- 反馈型：神经元输出经过若干神经元后反馈给输入端

### 神经网络工作方式

- 同步工作方式：各个神经元同时改变状态
- 异步工作方式：各个神经元一个一个改变状态

### 神经网络学习

神经网络学习指调整神经网络权值或结构，使输入输出有需要的特性

**Hebb学习规则**：若某一突触两段神经元同时处于兴奋状态，则该连接权值应该增强：
$$
w_{ij}(k+1)=w_{ij}(k)+\alpha y_i(k)y_j(k)\qquad (\alpha > 0)
$$

## BP神经网络

### BP神经网络结构

![bp str](/assets/ai_53.png)

**BP神经网络**有$m$层，其中：

- 第一层为输入层（$X=[x_1, x_2, \cdots, x _ {p_1}]^\mathrm{T}$）
- 最后一层为输出层（$Y=[y_1^m, y_2^m, \cdots, y _ {p_m}^m]^\mathrm{T}$）
- 中间各层为隐层

BP神经网络的输入输出的变换关系为：隐层$k$中非线性输入与输出关系记为$f_k (k=2,\cdots, m)$。第$k-1$层的第$j$个神经元到第$k$层的第$i$个神经元的连接权值为$w _ {ij}^k$，第$k$层第$i$神经元输入总和为$u_i^k$，输出为$y_i^k$，第$k$层神经元个数$p_k$，则映射关系为：

$$
u_i^k=\sum_{j=1}^{p_{k-1}}w_{ij}^ky_j^{k-1}-\theta_i=\sum_{j=0}^{p_{k-1}}w_{ij}^ky_j^{k-1}\\
y_i^k=f(u_i^k)
$$

"+1"的圆圈为**偏置节点**：没有输入，输出总为+1. 

$$
y_0^{k-1}=\theta_i\\
w_{i0}^k=-1
$$

### BP神经网络工作过程

BP神经网络包括两个阶段：

**A.** 网络训练阶段：

给定$N$组输入输出样本：

$$
X_si=[x_{si1}, x_{si2}, \cdots, x_{sip_1}]^\mathrm{T}\\
D_i = [d_{i1}, d_{i2}, \cdots, d_{ip_m}]^\mathrm{T}\\
i=1,2,\cdots, N
$$

对网络连接权进行学习和调整，以期望网络能够实现给定样本的输入输出映射关系。

**B.** 工作阶段：把实验数据/实际数据输入到网络，网络在误差范围内预测计算出结果。

### BP学习算法

由**Kolmogorov定理** （**BP定理**）：给定任意$\varepsilon > 0$，对于任意的连续函数，存在一个三层前向神经网络，它可以在任意$\varepsilon$平方误差精度内逼近连续函数。

学习算法包括两部分：

- 正向传播：输入信息由输入层传至隐层，最终在输出层输出。
- 反向传播：修改各层神经元的权值，使误差信号最小。

![bp algorithm](/assets/ai_54.png)

定义目标函数为：

$$
J=\frac{1}{2}\sum_{j=1}^{p_m}(y_j^m-d_j)^2
$$

BP学习算法用公式表示为：

$$
\Delta w_{ij}^{k-1}=-\varepsilon\frac{\partial J}{\partial w_{ij}^{k-1}}=\frac{\partial J}{\partial u_i^k}\frac{\partial}{\partial w_{ij}^{k-1}}\Big(\sum_jw_{ij}^{k-1}y_j^{k-1}\Big)=-\varepsilon d_i^k y_j^{k-1}\\
$$

输出层连接权调整公式为：

$$
d_i^m=\frac{\partial J}{\partial y_i^m}\frac{\partial y_i^m}{\partial u_i^m}=(y_i^m-y_{si})f^\prime_m(u_i^m)
$$

隐层连接权调整公式为：

$$
d_i^k=\frac{\partial J}{\partial y_i^k}\frac{\partial y_i^k}{\partial u_i^k}=\sum_l \frac{\partial J}{\partial u_l^{k+1}}\frac{\partial u_l^{k+1}}{\partial y_i^k}f^\prime_k(u_i^k)=f^\prime_k(u_i^k)\sum_{l}d_l^{k+1}w_{li}^k
$$

### BP学习算法实现

（1）初始化：对所有连接权和阈值赋以随机任意小值；

$$
w_{ij}^k(t), \theta_i^k(t)
$$

（2） 从 $N$ 组输入输出样本中取一组样本：$x=[x_1, x_2, \cdots,  x _ {p_1}]^\mathrm{T}, d=[d_1, d_2,\cdots, d _ {p_m}]^\mathrm{T}$, 把输入信息$x=[x_1, x_2, \cdots,  x _ {p_1}]^\mathrm{T}$输入到BP网络中。

（3）正向传播：计算各层节点的输出：

$$
y_i^k\qquad(i=1, \cdots, p_k; k = 1, \cdots, m)
$$

（4）计算网络的实际输出与期望输出的误差：

$$
e_i = y_i - y_i^m\qquad(i = 1, \cdots, p_m)
$$

（5）反向传播：从输出层方向计算到第一个隐层，按连接权值修正公式向减小误差方向调整网络的各个连接权值。

（6）让$t+1\rightarrow t$，取出另一组样本重复（2）-（5），直到 $N$ 组输入输出样本的误差达到要求时为止。 


![bp implementation](/assets/ai_55.png)

## Hopfield神经网络

TODO 😢

## 卷积神经网络

![cnn](/assets/ai_50.png)

**卷积神经网络** (CNN) 是一个多层的神经网络，每层由多个二维平面组成，而每个平面由多个独立神经元组成。

- C层为特征提取层（卷积层）：通过卷积运算进行特征提取。卷积神经网络中的关键技术包括——局部链接、权值共享、多卷积核。
- S层是特征映射层（下采样层）：对不同位置的特征进行聚合统计,称为**池化**（下采样）。池化常用方法包括：
  - 平均池化
  - 最大池化

![convolution](/assets/ai_51.png)

## 胶囊网络

TODO 😭