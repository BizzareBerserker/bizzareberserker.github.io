---
layout: post
title: "概率论其他课题"
tag: probability
category: essay
---

## Markov链

**Markov链**：考虑一个随机变量序列$X_0,X_1,X_2,\cdots$​​，每个变量的可能取值集合为$\{0,1,\cdots, M\}$​​（称为**状态空间**）​，称$X_n=i$​为系统在时刻$n$​处于状态$i$​​；定义**转移概率**$P_{ij}$​为在某一时刻系统处于状态$i$​时系统在下一时刻处于状态$j$​的概率，即
$$
P\{X_{n+1}=j|X_n=i,X_{n-1}=i,\cdots,X_1=i,X_0=i\}=P_{ij}
$$

- **转移概率**：满足
  $$
  P_{ij}\ge0\qquad \sum_{j=0}^MP_{ij}=1	
  $$
  同样，$P_{ij}^n$可以理解为从状态$i$经过$n$步到达状态$j$的概率

- **转移概率矩阵**：由转移概率组成的矩阵，其中第i行代表当前状态，第j列代表目标状态
  $$
  \begin{Vmatrix}
  P_{00}&P_{01}&\cdots&P_{0M}\\
  P_{10}&P_{11}&\cdots&P_{1M}\\
  \vdots&\vdots&&\vdots\\
  P_{M0}&P_{M1}&\cdots&P_{MM}
  \end{Vmatrix}
  $$

- **联合分布列**：直到转移概率后可以计算$X_0,X_1,\cdots$的联合概率分布列
  $$
  P\{X_n=i_n,X_{n-1}=i_{n-1},\cdots,X_1=i_1,X_0=i_0\}=P\{X=i_0\}P_{i_0i_1}P_{i_1i_2}\cdots P_{i_{n-1}i_n}
  $$

**Chapman-Kolgomorov方程**：
$$
P_{ij}^n=\sum_{k=0}^MP_{ir}^kP_{rj}^{n-k}\qquad\forall r:\ 0<r<n
$$
**极限概率**：对于可遍历的Markov链，有
$$
\pi_j=\lim_{n\to\infin}P_{ij}^n
$$
存在，并且$\pi_j(0\le j\le M)$是下列方程组的唯一非负解：
$$
\pi_j=\sum_{k=0}^M\pi_kP_{kj}\\
\sum_{j=0}^M\pi_j=1
$$

- **可遍历的Markov链**：如果$\forall i,j=0,1,\cdots,M\quad\exists n>0$，使得$P_{ij}^n>0$，则称该Markov链可遍历
- **极限概率的另一种解释**：当$n$充分大时，可以把$\pi_j$理解为系统处于状态$j$所占的时刻所占的长程比例

## 熵

**惊奇**：惊奇$S(p)$​​​是一个定义在$0<p\le1$​​​上的函数，它是对惊奇程度的量化。$S(p)$满足如下四条公理：

1. $S(1)=0$
2. $S(p)\downarrow$ (严格递减)
3. $S(p)$是一个连续函数
4. $S(pq)=S(p)+S(q)\qquad0<p\le1,0<q\le1$

给出$S(p)$​的表达式：
$$
S(p)=-C\log p
$$
通常情况下取$C=1$，单位为比特bit。

- 性质：（对数性质）
  $$
  S(p^x)=xS(p)
  $$
  
- 

**熵**：在信息论中，$H(X)$​​表示观测到$X$​​的值后所接受到的平均信息量，也可以认为熵时$X$​的不确定程度或得知$X$​值后所获得的平均经济程度。考虑随机变量$X$的取值范围$x_1,\cdots,x_n$，每个取值的概率为$p_1,\cdots,p_n$，则定义信息熵为
$$
H(X)=-\sum_{i=1}^np_i\log p_i
$$
**联合信息熵**：考虑两个随机变量$X,Y$，分别取值为$x_1,x_2,\cdots,x_n$和$y_1,y_2,\cdots,y_m$，其联合分布列为
$$
p(x_i,y_j)=P\{X=x_i,Y=y_j\}
$$
二者的联合信息熵为
$$
H(X,Y)=-\sum_i\sum_jp(x_i,y_j)\log p(x_i,y_j)
$$
进一步，我们可以定义**条件信息熵**如下：
$$
H_{Y=y_j}(X)=-\sum_i p(x_i|y_j)\log p(x_i|y_j)
$$

- *命题*：
  $$
  H(X,Y)=H(Y)+H_Y(X)
  $$
  特别地，如果$X,Y$独立，则
  $$
  H(X,Y)=H(Y)+H(X)
  $$

- $$
  H_Y(X)\le H(X)
  $$

  等号成立的$\Leftrightarrow$条件为$X,Y$独立

## 编码定理

**无噪声编码定理**：设$X$取值于$\{x_1,x_2,\cdots,x_N\}$，其相应的概率为$\{p_1,p_2,\cdots,p_N\}$，设有一个编码系统，将$x_i$编码为$n_i$位的二进制序列，则其平均长度
$$
\sum_{i=1}^Nn_ip(x_i)\ge H(X)
$$
显然，对于大部分随机向量，我们都能找到一个编码系统，使得其平均长度$L=\sum n_ip(x_i)$满足$H(X)\le L<H(X)+1$. 

- **引理**：设$X$​​​​的可能取值为$x_1,x_2,\cdots,x_N$​​​​，把它们编成长度为$n_1,n_2,\cdots,n_N$​​​​的无前缀0-1序列，其充要条件为
  $$
  \sum_{i=1}^N\Big(\frac{1}{2}\Big)^{n_i}
  $$

**有噪声编码定理**：$\exists C$​​，使得对于$\forall R<C$​和$\forall\varepsilon>0$​，存在一个编码译码系统能够以平均速率$R\text{ bit/s}$​​传输信号且误差概率小于$\varepsilon$​. 最大的$C$​值称之为**通道容量**，记作$C^*$​​；在二进制对称系统中，它等于
$$
C^*=1+p\log p+(1-p)\log(1-p)
$$

- **二进制对称系统**：假设每个信号单位以概率$p$从地点A传输到地点B，且每个信号传输的错误概率时独立的，这样的系统称作二进制对称系统；对于这样的系统，可以通过信号重复并在译码时取多数的方式来降低误差概率。

$$

$$

